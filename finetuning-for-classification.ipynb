{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3418b1",
   "metadata": {},
   "source": [
    "We will cover - \n",
    "- different llm fine tuning approaches\n",
    "- prepare dataset for text classification\n",
    "- fine tune llm for spam message detection\n",
    "- evaluate the accuracy of a fine-tuned llm classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafa3aa",
   "metadata": {},
   "source": [
    "most common ways of finetuning - instruction fine tuning and classification fine tuning.\n",
    "We will learn about Classification Fine Tuning in this chapter6.\n",
    "\n",
    "The downside is its restricted to predicting classes it has encountered during training.\n",
    "\n",
    "Clas. ft model can be seen as a specialized and generally it is easier to develop a specialized model than a generalist model.\n",
    "\n",
    "![ft1](images/ft-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbd732",
   "metadata": {},
   "source": [
    "<h4>Stage1</h4>\n",
    "\n",
    "1. Download the dataset, Balance it (oversample or undersample), Give it numerical class representation (0 or normal, 1 for spam, similar to converting tokens to token_ids), split it training and evaluation subsets.\n",
    "\n",
    "2. Creating data loaders - we used a sliding window approach to generate uniformly sized text chuncks to make batches for efficient model training.\n",
    "Here also for such batching, we can decide on the chunk size based on 2 approaches -\n",
    " - truncate all messages to the lenfth of the shortest message in the dataset\n",
    " - pad all messages to the length of the longest message in the dataset\n",
    "\n",
    "    We go with the second approach so that we dont have any info loss, and for it we pad with the token_id with <|endoftext|>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e51d9",
   "metadata": {},
   "source": [
    "<h4>Stage 2</h4>\n",
    "\n",
    "1. Adding a classification head - we replace the original output layer, which maps the hidden representation to a vocab of 50,257, with a smaller output layer that maps to two classes: 0 \"notspam\" and 1 \"spam\"\n",
    "\n",
    "    note - When fine-tuning a pretrained language model, you often don’t need to update all layers. Lower layers learn general language features that transfer well across tasks, while higher layers capture task-specific patterns. Fine-tuning only the top (output-side) layers is usually enough to adapt the model, and it’s more computationally efficient than updating the entire network.\n",
    "\n",
    "2. Our GPT Model architecture has 12 repeated transformers blocks, we only keep the output layer, final layernorm and the last transformer block as trainable. Remaining 11 transformers blocks and the embedding layers are kept non-trainable.\n",
    "\n",
    "3. Now the last layer outputs 2 columns (2 dim) instead of 50k (vocab size) and we only have to work with the last token (last row), reason being - because of the causal attention mask, the last token in the sequence accumulates the most information since it is the only token with access to the data from all the prev tokens.\n",
    "\n",
    "![ft2](images/ft-2.png)\n",
    "\n",
    "4. Before implimenting evalution utilities, we must first convert the model outputs into class label predictions\n",
    "\n",
    "![ft3](images/ft-3.png)\n",
    "\n",
    "5. Define the loss function, and select logits of just the last tokens for this cross entropy loss function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
