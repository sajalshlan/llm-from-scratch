{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64627db",
   "metadata": {},
   "source": [
    "![GPT](images/gpt-arch-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import TransformerBlock, LayerNorm\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.transformerlayers = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"number_of_layers\"])])\n",
    "        \n",
    "        self.finalnorm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        token_embeddings = self.tok_emb(input_idx)\n",
    "        positional_embeddings = self.pos_emb(torch.arange(seq_len, device=input_idx.device))\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformerlayers(x)\n",
    "        x = self.finalnorm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cffb60",
   "metadata": {},
   "source": [
    "Weight Tying \n",
    "\n",
    "a concept to save overall memory footprint and computation, used in original GPT2 architecture, where the token embedding weights are used in its output layer, otherwise it would've been 163M params instead of 124M.\n",
    "\n",
    "however, modern llm architectures use separate weights at these places as it gives better model training and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34725, 40702, 48254, 29693, 29342,  8794, 17406,  3440, 29527, 31767],\n",
      "        [24866, 40190, 14001, 34217,  3826, 17485, 22542, 25847,  3891,  4278]])\n",
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 50257])\n",
      "\n",
      "Model has 163,009,536 parameters\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 1024,   # Context length\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"num_heads\": 12,          # Number of attention heads\n",
    "    \"number_of_layers\": 12,   # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "\n",
    "# Example input (batch_size=2, seq_len=10)\n",
    "input_ids = torch.randint(0, GPT_CONFIG[\"vocab_size\"], (2, 10))\n",
    "print(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f7007",
   "metadata": {},
   "source": [
    "**Generating Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b923fcb",
   "metadata": {},
   "source": [
    "![](images/gpt-arch-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4278d45",
   "metadata": {},
   "source": [
    "We will implement Greedy Decoding, which is the most straightforward way an LLM generates text. It predicts one token at a time by always choosing the single most likely next word.\n",
    "\n",
    "There are more algorithms of generating text that brings in creativity by upating the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bca1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        #limit to our model's context_length\n",
    "        idx_possible_in_our_context = idx[:, -context_size:]\n",
    "\n",
    "        #now inferencing to get the result\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_possible_in_our_context)\n",
    "        \n",
    "        # now we only want to work with the las row as it contains the newly generated word; (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1 ,:]\n",
    "\n",
    "        # apply softmax to get probablities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # not pick only the token id of highest probability term\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae1117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 23515, 21842, 12754, 31479, 17159,  8621]])\n",
      "Output length: 10\n",
      "Hello, I amulus Dowacularï¿½ Along Sep\n"
     ]
    }
   ],
   "source": [
    "# Testing, but i need tokenizer for it\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
