{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64627db",
   "metadata": {},
   "source": [
    "![GPT](gpt-arch-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77fc91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import TransformerBlock, LayerNorm\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.transformerlayers = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"number_of_layers\"])])\n",
    "        self.finalnorm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        token_embeddings = self.tok_emb(input_idx)\n",
    "        positional_embeddings = self.pos_emb(torch.arange(seq_len, device=input_idx.device))\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformerlayers(x)\n",
    "        x = self.finalnorm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cffb60",
   "metadata": {},
   "source": [
    "Weight Tying \n",
    "\n",
    "a concept to save overall memory footprint and computation, used in original GPT2 architecture, where the token embedding weights are used in its output layer, otherwise it would've been 163M params instead of 124M.\n",
    "\n",
    "however, modern llm architectures use separate weights at these places as it gives better model training and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c5bd",
   "metadata": {},
   "source": [
    "![](/images/gpt-arch-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c8283",
   "metadata": {},
   "source": [
    "[](/images/gpt-arch-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e5a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 50257])\n",
      "\n",
      "Model has 163,009,536 parameters\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 1024,   # Context length\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"num_heads\": 12,          # Number of attention heads\n",
    "    \"number_of_layers\": 12,   # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "\n",
    "# Example input (batch_size=2, seq_len=10)\n",
    "input_ids = torch.randint(0, GPT_CONFIG[\"vocab_size\"], (2, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
