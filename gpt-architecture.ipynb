{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64627db",
   "metadata": {},
   "source": [
    "![GPT](images/gpt-arch-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import TransformerBlock, LayerNorm\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.transformerlayers = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"number_of_layers\"])])\n",
    "        \n",
    "        self.finalnorm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        token_embeddings = self.tok_emb(input_idx)\n",
    "        positional_embeddings = self.pos_emb(torch.arange(seq_len, device=input_idx.device))\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformerlayers(x)\n",
    "        x = self.finalnorm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cffb60",
   "metadata": {},
   "source": [
    "Weight Tying \n",
    "\n",
    "a concept to save overall memory footprint and computation, used in original GPT2 architecture, where the token embedding weights are used in its output layer, otherwise it would've been 163M params instead of 124M.\n",
    "\n",
    "however, modern llm architectures use separate weights at these places as it gives better model training and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[48456, 25800, 42114, 10130, 24711, 47351,  3781, 36012, 16549, 23321],\n",
      "        [42968, 46211, 23438, 44343,  3518,  9778, 20626, 32497, 42517, 33764]])\n",
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 50257])\n",
      "\n",
      "Model has 163,009,536 parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG)\n",
    "\n",
    "# Example input (batch_size=2, seq_len=10)\n",
    "input_ids = torch.randint(0, GPT_CONFIG[\"vocab_size\"], (2, 10))\n",
    "print(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f7007",
   "metadata": {},
   "source": [
    "**Generating Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b923fcb",
   "metadata": {},
   "source": [
    "![](images/gpt-arch-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4278d45",
   "metadata": {},
   "source": [
    "We will implement Greedy Decoding, which is the most straightforward way an LLM generates text. It predicts one token at a time by always choosing the single most likely next word.\n",
    "\n",
    "There are more algorithms of generating text that brings in creativity by upating the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae1117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 45495, 21824, 25484, 28517, 24313, 25730]])\n",
      "Output length: 10\n",
      "Hello, I amï¿½ grains Invasion harmedodo Sherlock\n"
     ]
    }
   ],
   "source": [
    "from modules import generate_text_simple\n",
    "# Testing, but i need tokenizer for it\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
