{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38b0b2d",
   "metadata": {},
   "source": [
    "we will cover - \n",
    "\n",
    "- instruction finetuning of llms\n",
    "- dataset prep for supervised instruction finetuning\n",
    "- finetuning the llm\n",
    "- extracting and evaluating llm-generated instruction responses\n",
    "\n",
    "![ft4](images/ft-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a97dd9",
   "metadata": {},
   "source": [
    "<h4>Stage 1</h4>\n",
    "\n",
    "1. 2 different prompt styles to format the dataset into - Alpaca and Phi3. Alpaca was one of the first publicly detailed instruction fine tuning and more famous, later came Microsoft's Phi3. We use Alpaca. Also divide dataset into training, testing and validation.\n",
    "\n",
    "![ft5](images/ft-5.png)\n",
    "\n",
    "2. Training Batches - writing custom collate functions pads the training examples in each batch to the same length while allowing different batches to hae different lengths -> minimizes unnesessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.\n",
    "\n",
    "We replace all the end-of-text token with a special token (-100 here) to exclude these padding tokens from contributing to the training loss calculation, we replace all except the first end-of-text token to let the model know when to stop though. Using -100 as the default setting of cross entropy function in pytorch is - ignore_index=-100; ignoring targets labelled with -100.\n",
    "\n",
    "It is also common to mask out instructions in the targed tokens when calculating loss so that the model does not memorize instructions - reducing overfitting. sometimes its done, sometimes not, we are not going to currently.\n",
    "\n",
    "![ft6](images/ft-6.png)\n",
    "![ft7](images/ft-7.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
