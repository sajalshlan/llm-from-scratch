{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3418b1",
   "metadata": {},
   "source": [
    "We will cover - \n",
    "- different llm fine tuning approaches\n",
    "- prepare dataset for text classification\n",
    "- fine tune llm for spam message detection\n",
    "- evaluate the accuracy of a fine-tuned llm classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafa3aa",
   "metadata": {},
   "source": [
    "most common ways of finetuning - instruction fine tuning and classification fine tuning.\n",
    "We will learn about Classification Fine Tuning in this chapter6.\n",
    "\n",
    "The downside is its restricted to predicting classes it has encountered during training.\n",
    "\n",
    "Clas. ft model can be seen as a specialized and generally it is easier to develop a specialized model than a generalist model.\n",
    "\n",
    "![ft1](../images/ft-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbd732",
   "metadata": {},
   "source": [
    "<h4>Stage1</h4>\n",
    "\n",
    "1. Download the dataset, Balance it (oversample or undersample), Give it numerical class representation (0 or normal, 1 for spam, similar to converting tokens to token_ids), split it training and evaluation subsets.\n",
    "\n",
    "2. Creating data loaders - we used a sliding window approach to generate uniformly sized text chuncks to make batches for efficient model training.\n",
    "Here also for such batching, we can decide on the chunk size based on 2 approaches -\n",
    " - truncate all messages to the lenfth of the shortest message in the dataset\n",
    " - pad all messages to the length of the longest message in the dataset\n",
    "\n",
    "    We go with the second approach so that we dont have any info loss, and for it we pad with the token_id with <|endoftext|>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e51d9",
   "metadata": {},
   "source": [
    "<h4>Stage 2</h4>\n",
    "\n",
    "1. Adding a classification head - we replace the original output layer, which maps the hidden representation to a vocab of 50,257, with a smaller output layer that maps to two classes: 0 \"notspam\" and 1 \"spam\"\n",
    "\n",
    "    note - When fine-tuning a pretrained language model, you often don’t need to update all layers. Lower layers learn general language features that transfer well across tasks, while higher layers capture task-specific patterns. Fine-tuning only the top (output-side) layers is usually enough to adapt the model, and it’s more computationally efficient than updating the entire network.\n",
    "\n",
    "2. Our GPT Model architecture has 12 repeated transformers blocks, we only keep the output layer, final layernorm and the last transformer block as trainable. Remaining 11 transformers blocks and the embedding layers are kept non-trainable.\n",
    "\n",
    "3. Now the last layer outputs 2 columns (2 dim) instead of 50k (vocab size) and we only have to work with the last token (last row), reason being - because of the causal attention mask, the last token in the sequence accumulates the most information since it is the only token with access to the data from all the prev tokens.\n",
    "\n",
    "![ft2](../images/ft-2.png)\n",
    "\n",
    "4. Before implimenting evalution utilities, we must first convert the model outputs into class label predictions\n",
    "\n",
    "![ft3](../images/ft-3.png)\n",
    "\n",
    "5. Define the loss function, and select logits of just the last tokens for this cross entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba144aef",
   "metadata": {},
   "source": [
    "<h4>Stage 3</h4>\n",
    "\n",
    "![ft8](../images/ft-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ec456",
   "metadata": {},
   "source": [
    "<h3>Code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f033609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.8\n",
      "numpy version: 2.4.1\n",
      "tiktoken version: 0.12.0\n",
      "torch version: 2.9.1\n",
      "tensorflow version: 2.20.0\n",
      "pandas version: 3.0.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",  # Plotting library\n",
    "        \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "        \"tiktoken\",    # Tokenizer\n",
    "        \"torch\",       # Deep learning library\n",
    "        \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "        \"pandas\"       # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fa378",
   "metadata": {},
   "source": [
    "<h4>Preparing dataset and creating dataloaders</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81fbea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/danofer/sarcasm\"\n",
    "zip_path = \"sarcasm.zip\"\n",
    "extracted_path = \"sarcasm\"\n",
    "data_file_path = Path(extracted_path) / \"train-balanced-sarcasm.csv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        return\n",
    "\n",
    "    # downloading the file\n",
    "    response = requests.get(url, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as out_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                out_file.write(chunk)\n",
    "\n",
    "    # unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"train-balanced-sarcasm\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downlaod and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bb786da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245000</th>\n",
       "      <td>0</td>\n",
       "      <td>I've worked for Bernie. I've donated to Bernie. When the primary finally reaches CA, I'll vote for Bernie. I would not vote for him as an independent. I have no interest in splitting the leftist vote and getting a Trump presidency. You're gonna get one if that vile woman is on the ballot, like it or not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39624</th>\n",
       "      <td>0</td>\n",
       "      <td>So you name one thing? That's \"dumbing\" it down? *what is inside the team site* hasn't changed. In fact, you are getting more functionality than you could before... but I guess since you can't rename the URL, the whole application got worse. 2nd - How do I delete the site?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373963</th>\n",
       "      <td>0</td>\n",
       "      <td>*cross fingers* Please be red herring Please be misleading Please be a punishment for decompilers Open box in a nutshell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211118</th>\n",
       "      <td>0</td>\n",
       "      <td>The most frustrating conversation I've ever had An middle-aged sister who I used to be kind of close to when I was in mentioned to my aunt a couple months ago that she'd like to see me again. I was kinda excited because I had the impression she was genuinely curious why I had left. So when she got back from her trip recently, we made plans. We met for coffee just yesterday and it went terribly. Basically the conversation went downhill when I stupidly mentioned I was living with my boyfriend now and she's says \"Oh... Well you know I can't really condone that...\" For the rest of the conversation, she tried to get me to admit that the only reason I left was because I felt I had been wronged by God and that I can only be happy if I come back. I tried to say tell her that I left because of my personal research and that I was very happy now that I'm no longer a JW. So when she realized she wasn't getting to me, she tried guilt tripping me. She said stuff like \"you're really hurting me by saying such disrespectful stuff about Jehovah and not recognizing the wonderful gifts he's given you\". Finally I got tired of her trying to claim I was \"hurting\" her so I ended the conversation by rolling my eyes and sarcastically saying \"Right, because you're the real victim here aren't you?\" And walked away. \"I can't condone that\" \"Since when do I need you to condone my life choices?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664996</th>\n",
       "      <td>0</td>\n",
       "      <td>The church handbook of instructions seems to think so. Spiritual polygamy is still (to a degree) being practiced. Its just temporal polygamy that is not allowed. Exactly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  \\\n",
       "245000      0   \n",
       "39624       0   \n",
       "373963      0   \n",
       "211118      0   \n",
       "664996      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "245000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I've worked for Bernie. I've donated to Bernie. When the primary finally reaches CA, I'll vote for Bernie. I would not vote for him as an independent. I have no interest in splitting the leftist vote and getting a Trump presidency. You're gonna get one if that vile woman is on the ballot, like it or not.  \n",
       "39624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            So you name one thing? That's \"dumbing\" it down? *what is inside the team site* hasn't changed. In fact, you are getting more functionality than you could before... but I guess since you can't rename the URL, the whole application got worse. 2nd - How do I delete the site?  \n",
       "373963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    *cross fingers* Please be red herring Please be misleading Please be a punishment for decompilers Open box in a nutshell  \n",
       "211118  The most frustrating conversation I've ever had An middle-aged sister who I used to be kind of close to when I was in mentioned to my aunt a couple months ago that she'd like to see me again. I was kinda excited because I had the impression she was genuinely curious why I had left. So when she got back from her trip recently, we made plans. We met for coffee just yesterday and it went terribly. Basically the conversation went downhill when I stupidly mentioned I was living with my boyfriend now and she's says \"Oh... Well you know I can't really condone that...\" For the rest of the conversation, she tried to get me to admit that the only reason I left was because I felt I had been wronged by God and that I can only be happy if I come back. I tried to say tell her that I left because of my personal research and that I was very happy now that I'm no longer a JW. So when she realized she wasn't getting to me, she tried guilt tripping me. She said stuff like \"you're really hurting me by saying such disrespectful stuff about Jehovah and not recognizing the wonderful gifts he's given you\". Finally I got tired of her trying to claim I was \"hurting\" her so I ended the conversation by rolling my eyes and sarcastically saying \"Right, because you're the real victim here aren't you?\" And walked away. \"I can't condone that\" \"Since when do I need you to condone my life choices?\"  \n",
       "664996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The church handbook of instructions seems to think so. Spiritual polygamy is still (to a degree) being practiced. Its just temporal polygamy that is not allowed. Exactly  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\",\")\n",
    "df_selected = df[[\"label\", \"comment\", \"parent_comment\"]]\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"label\"] = df_selected[\"label\"]\n",
    "df_new[\"text\"] = df_selected[\"parent_comment\"]+\" \"+df_selected[\"comment\"]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_new.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ea559d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text'], dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(df_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3906f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    505413\n",
      "1    505413\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_new[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29016ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5054\n",
       "1    5054\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = (\n",
    "    df_new\n",
    "    .groupby(\"label\")\n",
    "    .apply(lambda x: x.sample(frac=0.01, random_state=42))\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "df_reduced[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8581d6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>level_1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>772932</td>\n",
       "      <td>I love Sanders people. When 60% say wealth distribution in this country is unfair, or 50% say *Citizens United* is bad, it's all \"OMG see, the people are on our side.\" When 53% say they won't vote for a socialist, \"most\" of them are just ignorant people who don't know better. Pick a direction and stick with it, are polls to be relied on, or are the American people ignorant? Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>445608</td>\n",
       "      <td>Pawsitively relaxed Is there a subreddit for innocent thumbnails that look NSFW?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>354910</td>\n",
       "      <td>Probably skiddies that are too afraid/unskilled (most likely a combination of both) to take on a bigger target while they're on spring break. The truth sadly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>284811</td>\n",
       "      <td>huge geography freak here as well, SO many major differences there have been for myself, since you're talking about countries that you don't recognize, can you look at Africa again and tell me if you recognize any of these: Mauritania, Western Sahara, Burkina Faso, Eritrea and then Moldova in Europe. Thank you. Yes, all of those countries I remember.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>516997</td>\n",
       "      <td>Hearthstone? That's not an actually competitive game. They play BO3 despite it being so so luck focused. MTG doesn't have as many problems with RNG like HS does. In Poker they play hundreds of hands to decide who is the better player. The more they play the more the RNG matters less, and skill becomes more apparent. erm.. \"the more u play the less RNG matters\" .. So then you cant attribute critical strike being in the game to you being silver elo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103</th>\n",
       "      <td>1</td>\n",
       "      <td>547967</td>\n",
       "      <td>As amazing an experience this had to have been, this is not good at all for the animal Leaving the orphaned injured cheetah to die alone is much better for the animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10104</th>\n",
       "      <td>1</td>\n",
       "      <td>498424</td>\n",
       "      <td>This may just be me, but Sanders supporters seem to have a penchant for sexual metaphors. I don't particularly care for it. But its valid, because obviously she only got where she is because woman ,duh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10105</th>\n",
       "      <td>1</td>\n",
       "      <td>505928</td>\n",
       "      <td>Thanks big tobacco! I better stick with cigs, at least they've been PROVEN to kill me! the real meaning behind it, is to stop overpopulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10106</th>\n",
       "      <td>1</td>\n",
       "      <td>427952</td>\n",
       "      <td>SO, NASA Got Sick of all that Conspiracy Thing and Released over 10,000 Photos from the Apollo Moon Mission now that they can photoshop it well enough do do doooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10107</th>\n",
       "      <td>1</td>\n",
       "      <td>970028</td>\n",
       "      <td>Actually ps4 GPU is similar to GTX660, which costs only about $215 its actually closer to a 7850 which is cheaper.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  level_1  \\\n",
       "0          0   772932   \n",
       "1          0   445608   \n",
       "2          0   354910   \n",
       "3          0   284811   \n",
       "4          0   516997   \n",
       "...      ...      ...   \n",
       "10103      1   547967   \n",
       "10104      1   498424   \n",
       "10105      1   505928   \n",
       "10106      1   427952   \n",
       "10107      1   970028   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "0                                                                             I love Sanders people. When 60% say wealth distribution in this country is unfair, or 50% say *Citizens United* is bad, it's all \"OMG see, the people are on our side.\" When 53% say they won't vote for a socialist, \"most\" of them are just ignorant people who don't know better. Pick a direction and stick with it, are polls to be relied on, or are the American people ignorant? Yes  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                         Pawsitively relaxed Is there a subreddit for innocent thumbnails that look NSFW?  \n",
       "2                                                                                                                                                                                                                                                                                                           Probably skiddies that are too afraid/unskilled (most likely a combination of both) to take on a bigger target while they're on spring break. The truth sadly.  \n",
       "3                                                                                                         huge geography freak here as well, SO many major differences there have been for myself, since you're talking about countries that you don't recognize, can you look at Africa again and tell me if you recognize any of these: Mauritania, Western Sahara, Burkina Faso, Eritrea and then Moldova in Europe. Thank you. Yes, all of those countries I remember.  \n",
       "4      Hearthstone? That's not an actually competitive game. They play BO3 despite it being so so luck focused. MTG doesn't have as many problems with RNG like HS does. In Poker they play hundreds of hands to decide who is the better player. The more they play the more the RNG matters less, and skill becomes more apparent. erm.. \"the more u play the less RNG matters\" .. So then you cant attribute critical strike being in the game to you being silver elo?  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...  \n",
       "10103                                                                                                                                                                                                                                                                                               As amazing an experience this had to have been, this is not good at all for the animal Leaving the orphaned injured cheetah to die alone is much better for the animal  \n",
       "10104                                                                                                                                                                                                                                                           This may just be me, but Sanders supporters seem to have a penchant for sexual metaphors. I don't particularly care for it. But its valid, because obviously she only got where she is because woman ,duh.  \n",
       "10105                                                                                                                                                                                                                                                                                                                         Thanks big tobacco! I better stick with cigs, at least they've been PROVEN to kill me! the real meaning behind it, is to stop overpopulation  \n",
       "10106                                                                                                                                                                                                                                                                                                   SO, NASA Got Sick of all that Conspiracy Thing and Released over 10,000 Photos from the Apollo Moon Mission now that they can photoshop it well enough do do doooo  \n",
       "10107                                                                                                                                                                                                                                                                                                                                                   Actually ps4 GPU is similar to GTX660, which costs only about $215 its actually closer to a 7850 which is cheaper.  \n",
       "\n",
       "[10108 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbc9ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # shuffling first\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    train_end = int(len(df)*train_frac)\n",
    "    validation_end = train_end + int(len(df)* validation_frac)\n",
    "    \n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end: validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(df_reduced, 0.7, 0.1)\n",
    "\n",
    "train_df.to_csv(\"../data/sarcasm/train.csv\", index=None)\n",
    "validation_df.to_csv(\"../data/sarcasm/validation.csv\", index=None)\n",
    "test_df.to_csv(\"../data/sarcasm/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7d7ce",
   "metadata": {},
   "source": [
    "Dataloader part - padding all messages to match the length of longest sequence in a batch, using gpt2 to train so will use its padding token only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6e9011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "835558c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Convert text to string and handle NaN values\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(str(text)) if pd.notna(text) else tokenizer.encode(\"\") \n",
    "            for text in self.data[\"text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # we will pad, but we also need to truncate the long ones to a given max length(future compatability, if given)\n",
    "            self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts]\n",
    "\n",
    "        # now the main padding thing\n",
    "        self.encoded_texts = [encoded_text + [pad_token_id]* (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for text in self.encoded_texts:\n",
    "            length = len(text)\n",
    "            if length>max_length:\n",
    "                max_length = length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31e2f66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SarcasmDataset(\n",
    "    csv_file=\"../data/sarcasm/train.csv\",\n",
    "    max_length=1024,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c32ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SarcasmDataset(\n",
    "    csv_file=\"../data/sarcasm/validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SarcasmDataset(\n",
    "    csv_file=\"../data/sarcasm/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf348554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5afcc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 1024])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "792af6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 training batches\n",
      "127 validation batches\n",
      "253 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a231da5",
   "metadata": {},
   "source": [
    "<h4>Bringing our gpt2 architecture and uploading gpt2 weights in it</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0f935a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"number_of_layers\": 12, \"num_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"number_of_layers\": 24, \"num_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"number_of_layers\": 36, \"num_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"number_of_layers\": 48, \"num_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8993a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (transformerlayers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (finalnorm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import gpt2_download, gpt, weight_loader\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = gpt2_download.download_and_load_gpt2(model_size=model_size, models_dir=\"../models/gpt2\")\n",
    "\n",
    "model = gpt.GPTModel(BASE_CONFIG)\n",
    "weight_loader.load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bf18df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from modules import generation, data\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generation.generate_text_simple(\n",
    "    model=model,\n",
    "    idx=data.text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(data.token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4a23a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'sarcastic'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "'You have been specially selected to receive $1000 cash or a $2000 award.' 'You have been\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'sarcastic'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generation.generate_text_simple(\n",
    "    model=model,\n",
    "    idx=data.text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(data.token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fe690",
   "metadata": {},
   "source": [
    "<h4>Replacing the last layer to a Classification Layer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e04da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (transformerlayers): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (finalnorm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc704df1",
   "metadata": {},
   "source": [
    "the goal is to replace and finetune the last transformer block and final output layer, so lets freeze the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67cf962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f5d6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfc56053",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformerlayers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.finalnorm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "130fd8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[4598,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n",
      "Outputs:\n",
      " tensor([[[-1.5465,  0.9756],\n",
      "         [-3.7792,  7.4043],\n",
      "         [-2.3096,  6.4195],\n",
      "         [-3.6631,  4.2262]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# lets try to use it\n",
    "inputs = tokenizer.encode(\"do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d8702",
   "metadata": {},
   "source": [
    "Since we are heavily interested in the last token as its the only token that contains information about the other tokens (enriched of meaning from all the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "750915f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.6631,  4.2262]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c3889",
   "metadata": {},
   "source": [
    "<h4>Making an accuracy loader and a loss loader now which will be used when training starts</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "403b70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :] #logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return correct_predictions/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c4ad031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Training accuracy: 43.75%\n",
      "Validation accuracy: 52.50%\n",
      "Test accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8e69a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70d45527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd4cf0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.115\n",
      "Validation loss: 4.790\n",
      "Test loss: 4.573\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            # copying this optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        \n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # disables gradient, saves time and memory\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1daebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2cf21ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 50.99%\n",
      "Validation accuracy: 50.79%\n",
      "Test accuracy: 50.17%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309fb41",
   "metadata": {},
   "source": [
    "<h4>Inferencing one example to see how the model behaves</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1286f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_this_sarcasm_101(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval() # dropout off, batchnorm stops updating stats, \n",
    "\n",
    "    # you will get an input text, encode it, truncate it to max_length if given, pad it, convert to tensor and add a batch dimension, pass it to model with no_grad(), get the index of the logit that is max, return sarcasm if that index is 1\n",
    "\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length_for_this_model = model.pos_emb.weight.shape[0]\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length_for_this_model)]\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"funny\" if predicted_label == 1 else \"not so funny\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b0f627dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funny\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "text_2 = (\n",
    "    \"whatevver you say\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(is_this_sarcasm_101(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b31da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sarcasm_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}