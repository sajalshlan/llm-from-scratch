{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ee5bef",
   "metadata": {},
   "source": [
    "2. Normalization Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a7f84",
   "metadata": {},
   "source": [
    "output is forced into a standard distribution, without scale and shift the network would lose the ability to choose scale or offset destroying representational freedom.\n",
    "\n",
    "If the network decides normalization is harmful at some layer:\n",
    "\n",
    "γ → √variance\n",
    "β → mean\n",
    "\n",
    "Then:y≈x\n",
    "\n",
    "So LayerNorm does not force normalization — it offers it as an option.\n",
    "\n",
    "Also, the model can learn the params of scale and shift during backpropagation and adjust weights accordingly - giving freedom to express one feature more than the other and not forcing the activations to have 0 mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9883282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0663d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecc4d37",
   "metadata": {},
   "source": [
    "GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8934d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3854bba",
   "metadata": {},
   "source": [
    "GELU VS RELU\n",
    "\n",
    "**Point 1 — Smooth gradients:**\n",
    "ReLU has a sharp corner at zero, so a tiny change in weights can suddenly flip a neuron’s gradient from 1 to 0. This makes optimization noisy and unstable in very deep networks. GELU is smooth everywhere, so small weight changes lead to small, predictable gradient changes, which makes gradient descent more stable and easier to optimize.\n",
    "\n",
    "**Point 2 — No hard shut-off:**\n",
    "ReLU completely shuts neurons off for negative inputs (output and gradient both become zero), so those neurons stop contributing and learning. GELU instead gives small, non-zero outputs and gradients for negative inputs, allowing neurons to keep participating and learning, which improves gradient flow and overall training stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d328626",
   "metadata": {},
   "source": [
    "**FeedForward**\n",
    "\n",
    "feed forward layers are expanded in deep neural networks so that they can -\n",
    "\n",
    "1. learn richer representation by feature separation - easier to distinguish\n",
    "2. capture more patterns - since more room/neurons to express the features; feature mixing - the neurons can compute more combinations among input features\n",
    "3. bringing in non linearity here - helps the network to make decisions and learn these complex patterns via these non-linear shapes \n",
    "\n",
    "this is being used everywhere - transformers, vision transformers, MoE, CNNs.\n",
    "\n",
    "underlying principle remains the same - give the network room to compute complex transformations, then distill back to what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54c2a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model has 4,722,432 parameters\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4*config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*config[\"emb_dim\"], config[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 1024,   # Context length\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"num_heads\": 12,          # Number of attention heads\n",
    "    \"number_of_layers\": 12,   # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-Key-Value bias\n",
    "}\n",
    "ff_model = FeedForward(GPT_CONFIG)\n",
    "print(f\"\\nModel has {sum(p.numel() for p in ff_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab9b18",
   "metadata": {},
   "source": [
    "\n",
    "![](../images/feed-forward-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9bd1c",
   "metadata": {},
   "source": [
    "![](../images/feed-forward-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e66ef",
   "metadata": {},
   "source": [
    "**Skip Connections**\n",
    "\n",
    "deep nn, 2 problems - \n",
    "1. updates we do to the later layers aren’t very meaningful - because the inputs are too scrambled as so many random weight multiplications and activations to the actual input has turned it into a noise and the output/last layers has almost no signal/relevance to input layers\n",
    "2. updates we do at the early layer also aren’t very meaningful - because the gradients are also too scrambled by many multiplications \n",
    "\n",
    "we would like to -\n",
    "1. create a way where the inputs can arrive at the later layers and make the inputs meaningful\n",
    "2. loss gradients to arrive at early layer and make their updates more meaningful\n",
    "\n",
    "Residual Blocks - a collection of layers where data goes both - through and around using skip connections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786e1c3",
   "metadata": {},
   "source": [
    "**Making a full transformers block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee50a3",
   "metadata": {},
   "source": [
    "![T](../images/transformer-block-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44623303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (feedforward): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Model has 2,360,064 parameters\n",
      "\n",
      "Model has 4,722,432 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from modules import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = config[\"emb_dim\"],\n",
    "            d_out = config[\"emb_dim\"],\n",
    "            context_length = config[\"context_length\"],\n",
    "            num_heads = config[\"num_heads\"],\n",
    "            dropout = config[\"drop_rate\"],\n",
    "            qkv_bias = config[\"qkv_bias\"],\n",
    "        )\n",
    "        self.feedforward = FeedForward(config)\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connection = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        x = x + skip_connection\n",
    "\n",
    "        skip_connection = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + skip_connection\n",
    "\n",
    "        return x\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 1024,   # Context length\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"num_heads\": 12,          # Number of attention heads\n",
    "    \"number_of_layers\": 12,   # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "transformer = TransformerBlock(GPT_CONFIG)\n",
    "print(transformer)\n",
    "print(f\"\\nModel has {sum(p.numel() for p in transformer.attention.parameters()):,} parameters\")\n",
    "print(f\"\\nModel has {sum(p.numel() for p in transformer.feedforward.parameters()):,} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}