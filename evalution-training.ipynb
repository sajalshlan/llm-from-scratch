{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e9f5709",
      "metadata": {},
      "source": [
        "**To be covered**\n",
        "\n",
        "1. Calculating loss using backprop algo, setting testing and val datasets\n",
        "2. Pretraining and saving model weights\n",
        "3. Loading Openai GPT2 weights into our architecture\n",
        "\n",
        "![](images/eval-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ab35af",
      "metadata": {},
      "source": [
        "![eval-2](images/eval-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "941da121",
      "metadata": {},
      "source": [
        "**Generating Text Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27219186",
      "metadata": {},
      "source": [
        "1. unsqueeze(0) — Adding the \"Batch\" Dimension\n",
        "Models like GPT are designed to process many sentences at once (a \"batch\") to be efficient. Because of this, they always expect a 2D or 3D input, even if you are only sending them a single sentence.\n",
        "\n",
        "    Your input: [15496, 995] (Shape: [2]) — Just a simple list of words.\n",
        "\n",
        "    Model expects: [[15496, 995]] (Shape: [1, 2]) — A batch containing one sentence.\n",
        "\n",
        "    By calling .unsqueeze(0), you are telling PyTorch: \"Add a new dimension at the very beginning (index 0) so this looks like a batch of 1.\"\n",
        "\n",
        "2. squeeze(0) — Removing the \"Batch\" Dimension\n",
        "Once the model is done and gives you an output, it still includes that extra batch \"container.\" However, your tokenizer.decode() function doesn't know what a batch is—it just wants a flat list of numbers.\n",
        "\n",
        "    Model output: tensor([[15496, 995, ...]]) (Shape: [1, 12])\n",
        "\n",
        "    Decoder wants: [15496, 995, ...] (Shape: [12])\n",
        "\n",
        "    By calling .squeeze(0), you are saying: \"Take that outermost 'batch' dimension away so I can just get the list of word IDs back.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "71efdded",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output text -  Every effort moves youaser prices RPGPH ILCSifference ($ Morocco577 persecuted\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from modules import generate_text_simple, GPTModel, GPT_CONFIG_124M\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flattened = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flattened.tolist())\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()  \n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "\n",
        "print(\"output text - \", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef0ddc1",
      "metadata": {},
      "source": [
        "Gives gibberish as model is not yet trained, we need to train it and for that, we need to define an evalution metric, a framework which can let us know if the model is getting better, so lets define that loss\n",
        "\n",
        "The model training aims to increase softmax probabilites at the indices of the correct token ids. This is done by updating model weights using BackPropagation, and backprop requires a loss function which calculates the difference between model's predicted outputs and correct outputs.\n",
        "\n",
        "![eval3](images/eval-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625fce49",
      "metadata": {},
      "source": [
        "We compute the logits(output) from the model, apply softmax, get the probablity at the correct next token index and negative average log probabilities is the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "443958b4",
      "metadata": {},
      "source": [
        "#Perplexity\n",
        "A measure used along with cross entropy loss to evaluate the performance of models in tasks like language modelling. It tells how much the prob distribution of model outputs is away from the correct prob distribution. Given by torch.exp(loss), and similar to loss, less means better model. Signifies the effective vocab size about which the model is uncertain at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3faf5be9",
      "metadata": {},
      "source": [
        "<h2>Creating Loaders for test and val datasets</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cff696f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    text_data = response.text\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "628b308b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5795a5b9",
      "metadata": {},
      "source": [
        "Creating training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ab78ca20",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e81d798e",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d8be96",
      "metadata": {},
      "source": [
        "![train2](images/train-2.png)\n",
        "\n",
        "all the above steps are covered in pytorch's cross entropy loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "143ea9c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_loss_batch(input_batch, output_batch, model, device):\n",
        "    input_batch, output_batch = input_batch.to(device), output_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), output_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss=0\n",
        "\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i<num_batches:\n",
        "            loss=calculate_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss/num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c639b5db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device.\n",
            "0.6051034000184801 6.391407012939453\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Use PyTorch 2.9 or newer for stable mps results\n",
        "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
        "    if (major, minor) >= (2, 9):\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calculate_loss_loader(train_loader, model, device)\n",
        "    val_loss = calculate_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de42405",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "62cbca7b",
      "metadata": {},
      "source": [
        "<h2>Now comes the main training part</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3e5806de",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # setting to training mode, because it is also changed to eval mode down which stops dropout \n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # calculating loss gradients\n",
        "            optimizer.step() # update model weights based on gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # optional evalution step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "                \n",
        "        # print a sample text after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calculate_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calculate_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) \n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "335c983a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.924\n",
            "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.332\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.619, Val loss 7.042\n",
            "Ep 2 (Step 000015): Train loss 6.046, Val loss 6.596\n",
            "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.524, Val loss 6.508\n",
            "Ep 3 (Step 000025): Train loss 5.369, Val loss 6.378\n",
            "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
            "Ep 4 (Step 000030): Train loss 4.830, Val loss 6.263\n",
            "Ep 4 (Step 000035): Train loss 4.586, Val loss 6.285\n",
            "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 3.879, Val loss 6.130\n",
            "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
            "Ep 6 (Step 000045): Train loss 3.530, Val loss 6.183\n",
            "Ep 6 (Step 000050): Train loss 2.960, Val loss 6.123\n",
            "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
            "Ep 7 (Step 000055): Train loss 2.832, Val loss 6.150\n",
            "Ep 7 (Step 000060): Train loss 2.104, Val loss 6.133\n",
            "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
            "Ep 8 (Step 000065): Train loss 1.691, Val loss 6.186\n",
            "Ep 8 (Step 000070): Train loss 1.391, Val loss 6.230\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 1.059, Val loss 6.251\n",
            "Ep 9 (Step 000080): Train loss 0.800, Val loss 6.278\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
            "Ep 10 (Step 000085): Train loss 0.569, Val loss 6.373\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Training completed in 1.10 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs=10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer)\n",
        "\n",
        "end_time=time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
