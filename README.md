# LLM From Scratch

This repository contains my personal study notes and implementations while learning about attention mechanisms in Large Language Models. The images used in this project are from Sebastian Raschka's book "Build a Large Language Model (From Scratch)" published by Manning Publications. All image credits go to Sebastian Raschka.

## Structure

- `modules/` - Reusable Python modules for transformer components
- `self-attention.ipynb` - Self-attention and multi-head attention implementations
- `transformer_block.ipynb` - Transformer block with residual connections
- `gpt-architecture.ipynb` - Complete GPT model architecture
