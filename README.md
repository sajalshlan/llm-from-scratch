# LLM From Scratch

My personal study notes and implementations while learning about Large Language Models from Sebastian Raschka's book "Build a Large Language Model (From Scratch)".

## Notebooks

### Building the Architecture
1. `self-attention.ipynb` - Self-attention and multi-head attention
2. `transformer_block.ipynb` - Transformer block with residual connections
3. `gpt-architecture.ipynb` - Complete GPT model architecture

### Training & Generation
4. `evalution-training.ipynb` - Model evaluation and training loop
5. `decoding-strategies.ipynb` - Temperature scaling and top-k sampling
6. `loading-gpt2-weights.ipynb` - Loading pretrained GPT-2 weights

### Finetuning
7. `finetuning-for-classification.ipynb` - Finetuning for spam classification
8. `finetuning-for-instruct.ipynb` - Instruction finetuning

---

**Note:** The `modules/` directory contains reusable code extracted from the notebooks.
