{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9f5709",
   "metadata": {},
   "source": [
    "**To be covered**\n",
    "\n",
    "1. Calculating loss using backprop algo, setting testing and val datasets\n",
    "2. Pretraining and saving model weights\n",
    "3. Loading Openai GPT2 weights into our architecture\n",
    "\n",
    "![](images/eval-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab35af",
   "metadata": {},
   "source": [
    "![eval-2](images/eval-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941da121",
   "metadata": {},
   "source": [
    "**Generating Text Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27219186",
   "metadata": {},
   "source": [
    "1. unsqueeze(0) — Adding the \"Batch\" Dimension\n",
    "Models like GPT are designed to process many sentences at once (a \"batch\") to be efficient. Because of this, they always expect a 2D or 3D input, even if you are only sending them a single sentence.\n",
    "\n",
    "    Your input: [15496, 995] (Shape: [2]) — Just a simple list of words.\n",
    "\n",
    "    Model expects: [[15496, 995]] (Shape: [1, 2]) — A batch containing one sentence.\n",
    "\n",
    "    By calling .unsqueeze(0), you are telling PyTorch: \"Add a new dimension at the very beginning (index 0) so this looks like a batch of 1.\"\n",
    "\n",
    "2. squeeze(0) — Removing the \"Batch\" Dimension\n",
    "Once the model is done and gives you an output, it still includes that extra batch \"container.\" However, your tokenizer.decode() function doesn't know what a batch is—it just wants a flat list of numbers.\n",
    "\n",
    "    Model output: tensor([[15496, 995, ...]]) (Shape: [1, 12])\n",
    "\n",
    "    Decoder wants: [15496, 995, ...] (Shape: [12])\n",
    "\n",
    "    By calling .squeeze(0), you are saying: \"Take that outermost 'batch' dimension away so I can just get the list of word IDs back.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71efdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text -  Every effort moves youlord Simone intu intro caloric vaccinationestablished ClassificationPortland oats\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from modules import generate_text_simple, GPTModel, GPT_CONFIG_124M\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flattened = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flattened.tolist())\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()  \n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "\n",
    "print(\"output text - \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0ddc1",
   "metadata": {},
   "source": [
    "Gives gibberish as model is not yet trained, we need to train it and for that, we need to define an evalution metric, a framework which can let us know if the model is getting better, so lets define that loss\n",
    "\n",
    "The model training aims to increase softmax probabilites at the indices of the correct token ids. This is done by updating model weights using BackPropagation, and backprop requires a loss function which calculates the difference between model's predicted outputs and correct outputs.\n",
    "\n",
    "![eval3](images/eval-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fce49",
   "metadata": {},
   "source": [
    "We compute the logits(output) from the model, apply softmax, get the probablity at the correct next token index and negative average log probabilities is the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443958b4",
   "metadata": {},
   "source": [
    "#Perplexity\n",
    "A measure used along with cross entropy loss to evaluate the performance of models in tasks like language modelling. It tells how much the prob distribution of model outputs is away from the correct prob distribution. Given by torch.exp(loss), and similar to loss, less means better model. Signifies the effective vocab size about which the model is uncertain at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf5be9",
   "metadata": {},
   "source": [
    "<h2>Training an LLM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff696f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    text_data = response.text\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628b308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795a5b9",
   "metadata": {},
   "source": [
    "Creating training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab78ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81d798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
